{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"Tensorflow chat-bot model.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"JDovIIPsf24U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":530},"executionInfo":{"status":"error","timestamp":1597066288263,"user_tz":-330,"elapsed":7944,"user":{"displayName":"chinnu siddu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBmGL3qNrJDYiNSCIhcQBZqsvgBavBjeyPxlHx1A=s64","userId":"10015902151693891742"}},"outputId":"dd14f071-4bc3-4205-a4fc-09a8b5b0fe74"},"source":["# things we need for NLP\n","import nltk\n","from nltk.stem.lancaster import LancasterStemmer\n","stemmer = LancasterStemmer()\n","\n","# things we need for Tensorflow\n","import numpy as np\n","import tflearn\n","import tensorflow as tf\n","import random"],"execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-9e126770b7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# things we need for Tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tflearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_training_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tflearn/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tflearn/variables.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_arg_scope\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontrib_add_arg_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"RIHp05PHgR5Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":247},"executionInfo":{"status":"ok","timestamp":1597065271133,"user_tz":-330,"elapsed":38590,"user":{"displayName":"chinnu siddu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBmGL3qNrJDYiNSCIhcQBZqsvgBavBjeyPxlHx1A=s64","userId":"10015902151693891742"}},"outputId":"934a4b89-f8fa-44a6-8018-2c5852919ecd"},"source":["!pip install tflearn"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tflearn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/ec/e9ce1b52e71f6dff3bd944f020cef7140779e783ab27512ea7c7275ddee5/tflearn-0.3.2.tar.gz (98kB)\n","\r\u001b[K     |███▎                            | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.18.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (7.0.0)\n","Building wheels for collected packages: tflearn\n","  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tflearn: filename=tflearn-0.3.2-cp36-none-any.whl size=128208 sha256=1b3bcce937d4d20bdf6ed288a655ce7ae97b5895b45f3742530c73a6f682767e\n","  Stored in directory: /root/.cache/pip/wheels/d0/f6/69/0ef3ee395aac2e5d15d89efd29a9a216f3c27767b43b72c006\n","Successfully built tflearn\n","Installing collected packages: tflearn\n","Successfully installed tflearn-0.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G7Z52vSjf24i","colab_type":"code","colab":{}},"source":["# import our chat-bot intents file\n","import json\n","with open('intents.json') as json_data:\n","    intents = json.load(json_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvLKDdSPf24p","colab_type":"code","colab":{},"outputId":"8be2519d-16c3-4ffc-8d3f-731af296a68f"},"source":["words = []\n","classes = []\n","documents = []\n","ignore_words = ['?']\n","# loop through each sentence in our intents patterns\n","for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","        # tokenize each word in the sentence\n","        w = nltk.word_tokenize(pattern)\n","        # add to our words list\n","        words.extend(w)\n","        # add to documents in our corpus\n","        documents.append((w, intent['tag']))\n","        # add to our classes list\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])\n","\n","# stem and lower each word and remove duplicates\n","words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n","words = sorted(list(set(words)))\n","\n","# remove duplicates\n","classes = sorted(list(set(classes)))\n","\n","print (len(documents), \"documents\")\n","print (len(classes), \"classes\", classes)\n","print (len(words), \"unique stemmed words\", words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["27 documents\n","9 classes ['goodbye', 'greeting', 'hours', 'mopeds', 'opentoday', 'payments', 'rental', 'thanks', 'today']\n","48 unique stemmed words [\"'d\", \"'s\", 'a', 'acceiv', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'do', 'doe', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'hour', 'how', 'i', 'is', 'kind', 'lat', 'lik', 'mastercard', 'mop', 'of', 'on', 'op', 'rent', 'see', 'tak', 'thank', 'that', 'ther', 'thi', 'to', 'today', 'we', 'what', 'when', 'which', 'work', 'yo', 'you']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z1Zcs2SHf24w","colab_type":"code","colab":{}},"source":["# create our training data\n","training = []\n","output = []\n","# create an empty array for our output\n","output_empty = [0] * len(classes)\n","\n","# training set, bag of words for each sentence\n","for doc in documents:\n","    # initialize our bag of words\n","    bag = []\n","    # list of tokenized words for the pattern\n","    pattern_words = doc[0]\n","    # stem each word\n","    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n","    # create our bag of words array\n","    for w in words:\n","        bag.append(1) if w in pattern_words else bag.append(0)\n","\n","    # output is a '0' for each tag and '1' for current tag\n","    output_row = list(output_empty)\n","    output_row[classes.index(doc[1])] = 1\n","\n","    training.append([bag, output_row])\n","\n","# shuffle our features and turn into np.array\n","random.shuffle(training)\n","training = np.array(training)\n","\n","# create train and test lists\n","train_x = list(training[:,0])\n","train_y = list(training[:,1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"S-3aikmMf243","colab_type":"code","colab":{},"outputId":"d1ee3660-05fc-446c-f18d-6ee91944153c"},"source":["# reset underlying graph data\n","tf.reset_default_graph()\n","# Build neural network\n","net = tflearn.input_data(shape=[None, len(train_x[0])])\n","net = tflearn.fully_connected(net, 8)\n","net = tflearn.fully_connected(net, 8)\n","net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n","net = tflearn.regression(net)\n","\n","# Define model and setup tensorboard\n","model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n","# Start training (apply gradient descent algorithm)\n","model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n","model.save('model.tflearn')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.54986\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 1000 | loss: 0.54986 - acc: 0.9616 -- iter: 24/27\n","Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.50465\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 1000 | loss: 0.50465 - acc: 0.9654 -- iter: 27/27\n","--\n","INFO:tensorflow:/home/gk/gensim/notebooks/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TWgw9_nOf249","colab_type":"code","colab":{}},"source":["def clean_up_sentence(sentence):\n","    # tokenize the pattern\n","    sentence_words = nltk.word_tokenize(sentence)\n","    # stem each word\n","    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n","    return sentence_words\n","\n","# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n","def bow(sentence, words, show_details=False):\n","    # tokenize the pattern\n","    sentence_words = clean_up_sentence(sentence)\n","    # bag of words\n","    bag = [0]*len(words)  \n","    for s in sentence_words:\n","        for i,w in enumerate(words):\n","            if w == s: \n","                bag[i] = 1\n","                if show_details:\n","                    print (\"found in bag: %s\" % w)\n","\n","    return(np.array(bag))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXeULiTIf25L","colab_type":"code","colab":{},"outputId":"c81958a2-246e-4de1-fc93-50838061c266"},"source":["p = bow(\"is your shop open today?\", words)\n","print (p)\n","print (classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n"," 0 0 0 1 0 0 0 0 0 1 0]\n","['goodbye', 'greeting', 'hours', 'mopeds', 'opentoday', 'payments', 'rental', 'thanks', 'today']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NR0UYvVtf25T","colab_type":"code","colab":{},"outputId":"7a1e293e-5869-4f9b-fdd3-b13747777e99"},"source":["print(model.predict([p]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[5.111963474746517e-08, 0.00029038049979135394, 0.19395901262760162, 4.018096966262874e-10, 0.7987914085388184, 0.0005724101793020964, 8.153344310812827e-08, 5.96670907127006e-11, 0.0063865589909255505]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2yYlxG-Of25a","colab_type":"code","colab":{}},"source":["# save all of our data structures\n","import pickle\n","pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"],"execution_count":null,"outputs":[]}]}